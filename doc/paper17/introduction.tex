\section{Introduction}

Partitioned Global Address Space (PGAS) parallel programming models, such as
OpenSHMEM~\cite{openshmem-1.3}, Unified Parallel C~\cite{upc-13-spec}, Fortran
2008 Coarrays~\cite{reid:08,fortran-2008}, and the MPI Remote Memory Access
(RMA) interface MPI~\cite{mpi-forum:15}, have become popular as a means for
supporting distributed, shared global data.  In particular, PGAS models have
been shown to be especially effective when used to store dense arrays and
several models have been designed to support this type of
data~\cite{ga,kamil:14,ddi,niu:16}.

While PGAS models are commonly used with regularly structured data, they have
had limited success with irregular and sparse data.  Such data structures are
accessed indirectly and an additional indexing operation is needed to resolve
the location of a data element.  Indexing structures grow proportional to the
data size; thus, for the large data volumes that necessitate a PGAS solution, these
structures must also be distributed.  As a result, resolving the index of a data item
within the global address space is not possible using only local information.

Remote direct memory access (RDMA) read, write, and atomic update operations
are commonly supported by the high-speed fabrics used to construct HPC systems
and these operations are used to accelerate remote access operations performed
by PGAS models.  RDMA operations map well to remote array access operations,
since the locations to be accessed can be determined at the initiator of the
operation.  However, the indirect addressing required to remotely access sparse or
irregularly structured data presents challenges to existing RDMA interfaces
and can result in significant overheads.

% many applications that use sparse data structures are naturally expressed
% using logical addresses, such as a coordinate/level system in a spatial
% decompositon tree, etc.

% matching an irregular data structure's logical addressing scheme to a 
% traditional PGAS model can be challenging. 

% drawbacks/challenges with true PGAS models
%    - LA -> PA not feasible (i.e. grandchild of a tree node)
%    - indirections 
%       - (multiple fetches)
%       - directory structures
%    - adds latency for each access
%    - data layout may change over time
%    - caching is no silver bullet

% drawbacks with MPI / message passing approaches
%    - tags can be used to associate "logically addressed" locations
%    - MPI does not provide a fetch/get mechanism to communicate 
%      a tagged location

% this work describes an efficient implementation of a PGLAS system 
% that is built on networking mechanisms designed to support MPI
% communications

% our approach is based on some insights
%   - low-level networking systems provide direct support for
%     logical address systems 
%   - using lower-level systems reduce communication overhead
%   - applications can refer to globally shared data objects
%     using an addressing scheme that corresponds to the problem
%     at hand


% in particular, our work takes advantage of the Portals 4 network
% programming interface. Portals works great for a number of
% HPC communication middleware MPI and PGAS-based systems such as
% OpenMPI and UPC
%
% it's also great that Portals is designed to play nice with
% hardware offload systems

% in this paper we describe the implementation of a system that
% realizes a PGLAS with a PDHT and is implemented using the 
% Portals networking interface
% we make the following contributions:
%  - MPI matching interfaces can be used to implement a PGLAS
%  - PGLAS support can be easily run on hardware offload systems
%    that also support optimized MPI matching operations
%  - relying on the network layer to map matching criteria
%    to phys addr reduces the likelihood for collisions vs
%    a conventional hash table structure


The Portals 4.1 network programming interface~\cite{portals4} provides a
well-known interconnect programming model that is designed to permit efficient
hardware offload of communication processing. For example, the Bull\othertm{}
BXI\othertm{} interconnect~\cite{bxi} provides a hardware accelerated
implementation of the Portals 4.1 interface and the Cray\othertm{}
Seastar\othertm{} interconnect~\cite{brightwell:micro:06} provided a hardware
accelerated implementation of the Portals 3.0 interface.  Parallel programs
written using programming models that are built on the Portals interface will
be able to take advantage of network offload hardware.  The problem, of course,
is to map application-level data and task abstractions onto programming
constructs that are aligned with the communication operations supported by
low-level communications systems. 

Portals is designed as a low-level abstraction layer for the network interface
hardware. The Portals communication operations operations are
meant to support the efficient implementation of high-performance programming
interfaces and runtime systems that provide either message-passing semantics
(such as MPI), or one-sided, partitioned global address (PGAS) support. For
message-passing systems, Portals provides a {\em matching interface} that
allows communication middleware to robustly support a two-sided (matched sends
and receives) communication model. Alternatively, Portals also provides a {\em
non-matching interface} that sheds much of the complexity in matching messages
for efficient handling of one-sided communication operations found in PGAS
systems and languages.

Rather than considering the matching and non-matching interfaces as
the basis for higher-level communication systems, we have developed a
system that provides a distributed key/value storage that use Portals
primitives in a novel manner, leading towards a hardware accelerated
hash table system providing a PGLAS programming model. In this paper, we describe the implementation of a
parallel distributed hash table (PDHT) that is implemented directly
using the Portals programming interface. This work is based on the
following insights: (1) implementing data structures using Portals
leads to an opportunity to leverage efficient, hardware accelerated
implementations of Portals and (2) that operations on a hash-table
can be readily mapped to operations within
Portals that would be difficult to express exclusively within an
MPI-like system or a PGAS system.

This work makes the following contributions: We identify the
logically-addressed PGLAS paradigm as a novel extension to existing PGAS
programming concepts, which improves support for sparse and loosely structured
data.  We demonstrate the utility of this model through two representative
scientific applications, MADNESS and Meraculous, from the computational
chemistry and genomics domains, respectively.  We describe a new PGLAS model,
PDHT, a one-sided distributed hash table that is implemented using a novel
application of Portals matching interface operations. Lastly, we provide an
experimental validation, indicating that this approach can yield significant
performance improvements for these classes of applications.

% high performance interconnect hardware
%  - provides hardware optimized for use cases
%  - matching - > MPI
%  - one-sided PGAS

% insights

% contributions

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
