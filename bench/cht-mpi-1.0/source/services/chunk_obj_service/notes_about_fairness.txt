================================================================

From someone's lecture notes on the internet:

The MPI_WAITSOME and MPI_TESTSOME routines are similar to the
MPI_WAITANY and MPI_TESTANY routines, except that behaviour is
different if more than one com- munication can complete. In that case
MPI_WAITANY or MPI_TESTANY select a com- munication arbitrarily from
those which can complete, and returns status on that.  MPI_WAITSOME or
MPI_TESTSOME, on the other hand, return status on all commu- nications
which can be completed. They can be used to determine how many commu-
nications completed. It is not possible for a matched send/receive
pair to remain indefinitely pending during repeated calls to
MPI_WAITSOME or MPI_TESTSOME i.e.  the routines obey a fairness rule
to help prevent “starvation”.

================================================================

http://www.netlib.org/utk/papers/mpi-book/node43.html

Fairness

MPI makes no guarantee of fairness in the handling of
communication. Suppose that a send is posted. Then it is possible that
the destination process repeatedly posts a receive that matches this
send, yet the message is never received, because it is repeatedly
overtaken by other messages, sent from other sources. The scenario
requires that the receive used the wildcard MPI_ANY_SOURCE as its
source argument. MPI_ANY_SOURCE

Similarly, suppose that a receive is posted by a multi-threaded
process. Then it is possible that messages that match this receive are
repeatedly consumed, yet the receive is never satisfied, because it is
overtaken by other receives posted at this node by other threads. It
is the programmer's responsibility to prevent starvation in such
situations.

================================================================

http://www.mpi-forum.org/docs/mpi22-report/node62.htm#Node62

Advice to users.

The use of MPI_TESTSOME is likely to be more efficient than the use of
MPI_TESTANY. The former returns information on all completed
communications, with the latter, a new call is required for each
communication that completes.

A server with multiple clients can use MPI_WAITSOME so as not to
starve any client. Clients send messages to the server with service
requests. The server calls MPI_WAITSOME with one receive request for
each client, and then handles all receives that completed. If a call
to MPI_WAITANY is used instead, then one client could starve while
requests from another client always sneak in first. ( End of advice to
users.)

Advice to implementors.

MPI_TESTSOME should complete as many pending communications as possible. ( End of advice to implementors.)

Example Client-server code (starvation can occur).

[...]

Example Same code, using MPI_WAITSOME.

CALL MPI_COMM_SIZE(comm, size, ierr) 
CALL MPI_COMM_RANK(comm, rank, ierr) 
IF(rank .GT. 0) THEN         ! client code 
    DO WHILE(.TRUE.) 
       CALL MPI_ISEND(a, n, MPI_REAL, 0, tag, comm, request, ierr) 
       CALL MPI_WAIT(request, status, ierr) 
    END DO 
ELSE         ! rank=0 -- server code 
    DO i=1, size-1 
       CALL MPI_IRECV(a(1,i), n, MPI_REAL, i, tag, 
                      comm, request_list(i), ierr) 
    END DO 
    DO WHILE(.TRUE.) 
       CALL MPI_WAITSOME(size, request_list, numdone, 
                        indices, statuses, ierr) 
       DO i=1, numdone 
          CALL DO_SERVICE(a(1, indices(i))) 
          CALL MPI_IRECV(a(1, indices(i)), n, MPI_REAL, 0, tag, 
                       comm, request_list(indices(i)), ierr) 
       END DO 
    END DO 
END IF 

================================================================

http://www.mcs.anl.gov/research/projects/mpi/tutorial/gropp/node90.html#Node90

It is often desirable to wait on multiple requests. An example is a
master/slave program, where the master waits for one or more slaves to
send it a message.

# MPI_Waitall(count, array_of_requests, array_of_statuses)
# MPI_Waitany(count, array_of_requests, index, status)
# MPI_Waitsome(incount, array_of_requests, outcount, array_of_indices, array_of_statuses)
There are corresponding versions of test for each of these.

127 The MPI_WAITSOME and MPI_TESTSOME may be used to implement
master/slave algorithms that provide fair access to the master by the
slaves.

================================================================

http://www.mcs.anl.gov/research/projects/mpi/tutorial/gropp/node91.html#Node91

What happens with this program:

#include "mpi.h"
#include <stdio.h>
int main(argc, argv)
int argc;
char **argv;
{
int rank, size, i, buf[1];
MPI_Status status;

MPI_Init( &argc, &argv );
MPI_Comm_rank( MPI_COMM_WORLD, &rank );
MPI_Comm_size( MPI_COMM_WORLD, &size );
if (rank == 0) {
    for (i=0; i<100*(size-1); i++) {
	MPI_Recv( buf, 1, MPI_INT, MPI_ANY_SOURCE, 
                 MPI_ANY_TAG, MPI_COMM_WORLD, &status );
	printf( "Msg from %d with tag %d\n", 
                status.MPI_SOURCE, status.MPI_TAG );
	}
    }
else {
    for (i=0; i<100; i++) 
	MPI_Send( buf, 1, MPI_INT, 0, i, MPI_COMM_WORLD );
    }
MPI_Finalize();
return 0;
}

================================================================

http://www.mcs.anl.gov/research/projects/mpi/tutorial/gropp/node92.html#Node92

An parallel algorithm is fair if no process is effectively ignored. In
the preceeding program, processes with low rank (like process zero)
may be the only one whose messages are received.

MPI makes no guarentees about fairness. However, MPI makes it possible
to write efficient, fair programs.

================================================================

http://www.mcs.anl.gov/research/projects/mpi/tutorial/gropp/node93.html#Node93

One alternative is

#define large 128 
MPI_Request requests[large]; 
MPI_Status  statuses[large]; 
int         indices[large]; 
int         buf[large]; 
for (i=1; i<size; i++)  
    MPI_Irecv( buf+i, 1, MPI_INT, i, 
               MPI_ANY_TAG, MPI_COMM_WORLD, &requests[i-1] ); 
while(not done) { 
    MPI_Waitsome( size-1, requests, &ndone, indices, statuses ); 
    for (i=0; i<ndone; i++) { 
        j = indices[i]; 
        printf( "Msg from %d with tag %d\n",  
                statuses[i].MPI_SOURCE,  
                statuses[i].MPI_TAG ); 
        MPI_Irecv( buf+j, 1, MPI_INT, j, 
                   MPI_ANY_TAG, MPI_COMM_WORLD, &requests[j] ); 
        } 
    } 

================================================================
